{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks\n"
     ]
    }
   ],
   "source": [
    "cd /notebooks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixiedust database opened successfully\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"margin:10px\">\n",
       "            <a href=\"https://github.com/ibm-watson-data-lab/pixiedust\" target=\"_new\">\n",
       "                <img src=\"https://github.com/ibm-watson-data-lab/pixiedust/raw/master/docs/_static/pd_icon32.png\" style=\"float:left;margin-right:10px\"/>\n",
       "            </a>\n",
       "            <span>Pixiedust version 1.1.15</span>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from fastai import *\n",
    "from fastai.text import *   # Quick accesss to NLP functionality\n",
    "\n",
    "import torch.utils.data as data_utils\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data.sampler import Sampler\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import time\n",
    "import importlib\n",
    "import pixiedust\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPATH = Path('/data/WS/')\n",
    "PATH = Path('/data/sav/WS/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://dbressbuck/data/WS/evaluate.csv to ../data/WS/evaluate.csv\n",
      "download: s3://dbressbuck/data/WS/train.csv to ../data/WS/train.csv\n",
      "download: s3://dbressbuck/data/WS/classifier_weights_001.pth to ../data/WS/classifier_weights_001.pth\n",
      "download: s3://dbressbuck/data/WS/sent140_training.csv to ../data/WS/sent140_training.csv\n"
     ]
    }
   ],
   "source": [
    "#DOWNLOAD THE DATA\n",
    "!aws s3 cp --recursive s3://dbressbuck/data/WS /data/WS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(750000, 2)\n",
      "         sentiment                                            content\n",
      "477646           0  todays plans have been messed up as now not fe...\n",
      "133647           0  Nose is completely stuffed and throat is sore ...\n",
      "1196363          4                             Zero tolerence policy \n",
      "64522            0  How in the name of all that is holy did it get...\n",
      "523066           0  @tlmasonaea I wish I could, but I live a bit t...\n"
     ]
    }
   ],
   "source": [
    "#load data, look at size of training sets and test set, look at some samples\n",
    "df_trn = pd.read_csv(DATAPATH/'sent140_training.csv',encoding = \"ISO-8859-1\",names=['sentiment', 'id', 'date','flag','user','content'])\n",
    "df_trn= df_trn[['sentiment','content']]\n",
    "get_these=np.random.permutation(np.shape(df_trn)[0])\n",
    "df_trn=df_trn.iloc[get_these[:750000]]#take a sample of 750K, due to time constraints\n",
    "\n",
    "print(np.shape(df_trn))\n",
    "print(df_trn.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(743815, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>132481</th>\n",
       "      <td>0</td>\n",
       "      <td>Headache</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340713</th>\n",
       "      <td>0</td>\n",
       "      <td>isPlayer Has Died! Sorry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498708</th>\n",
       "      <td>0</td>\n",
       "      <td>I miss you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220156</th>\n",
       "      <td>0</td>\n",
       "      <td>I have a headache</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650089</th>\n",
       "      <td>0</td>\n",
       "      <td>Jogging, isnt REALLY that cool, especially if ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentiment                                            content\n",
       "132481          0                                          Headache \n",
       "340713          0                          isPlayer Has Died! Sorry \n",
       "498708          0                                        I miss you \n",
       "220156          0                                 I have a headache \n",
       "650089          0  Jogging, isnt REALLY that cool, especially if ..."
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#there are some duplicate tweets... Take a quick look at them\n",
    "print(np.shape(df_trn[~df_trn['content'].duplicated()]))\n",
    "df_duplicated=df_trn[df_trn['content'].duplicated()]\n",
    "df_duplicated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(743815, 2)\n"
     ]
    }
   ],
   "source": [
    "#remove duplicates... no reason to include these in training\n",
    "df_trn=df_trn[~df_trn['content'].duplicated()]\n",
    "print(np.shape(df_trn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         sentiment                                            content  str_len\n",
      "477646           0  todays plans have been messed up as now not fe...      101\n",
      "133647           0  Nose is completely stuffed and throat is sore ...       85\n",
      "1196363          4                             Zero tolerence policy        22\n",
      "64522            0  How in the name of all that is holy did it get...       58\n",
      "523066           0  @tlmasonaea I wish I could, but I live a bit t...       62\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f3a484273c8>]], dtype=object)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#create column to take a look at length of tweets, and create graph of distribution\n",
    "df_trn['str_len']=df_trn['content'].str.len()\n",
    "print(df_trn.head())\n",
    "df_trn.hist(column='str_len', bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time permitting, investigate if there's anything peculiar \n",
    "    #about the spike at the right side of the graph above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([371759.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,      0., 372056.]),\n",
       " array([0. , 0.4, 0.8, 1.2, 1.6, 2. , 2.4, 2.8, 3.2, 3.6, 4. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#look at distribution of labels... \n",
    "#2 categories, approximately equal number each\n",
    "print(df_trn['sentiment'].unique()) #unique labels\n",
    "plt.hist(list(df_trn['sentiment']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build new training set for semi-supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal: simulate common situation of: 1) Small labeled dataset, 2) Much larger unlabeled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeled set size: 14876\n",
      "unlabeled set size: 728939\n"
     ]
    }
   ],
   "source": [
    "#grab 2% of the dataset as our labeled dataset\n",
    "#the other 98% will serve as our 'unlabeled dataset'\n",
    "    #but we'll still keep the labels\n",
    "get_these=np.random.permutation(np.shape(df_trn)[0])\n",
    "split_here=round(.02*np.shape(df_trn)[0])\n",
    "df_labeled=df_trn.iloc[get_these[:split_here]]\n",
    "df_unlabeled=df_trn.iloc[get_these[split_here:]]\n",
    "print('labeled set size:', np.shape(df_labeled)[0])\n",
    "print('unlabeled set size:',np.shape(df_unlabeled)[0])\n",
    "assert np.shape(df_labeled)[0]+np.shape(df_unlabeled)[0]==np.shape(df_trn)[0], 'lengths dont add up'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set size: 2231\n",
      "valid set size: 2232\n",
      "train set size: 10413\n",
      "larger set size: 739352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "#create train, valid, and test set from labeled data\n",
    "get_these=np.random.permutation(np.shape(df_labeled)[0])\n",
    "split_here1=round(.15*np.shape(df_labeled)[0])\n",
    "split_here2=round(.3*np.shape(df_labeled)[0])\n",
    "df_labeled_test=df_labeled.iloc[get_these[:split_here1]]\n",
    "df_labeled_valid=df_labeled.iloc[get_these[split_here1:split_here2]]\n",
    "df_labeled_trn=df_labeled.iloc[get_these[split_here2:]]\n",
    "print('test set size:', np.shape(df_labeled_test)[0])\n",
    "print('valid set size:', np.shape(df_labeled_valid)[0])\n",
    "print('train set size:',np.shape(df_labeled_trn)[0])\n",
    "assert np.shape(df_labeled_test)[0]+np.shape(df_labeled_valid)[0]+np.shape(df_labeled_trn)[0]==np.shape(df_labeled)[0], 'lengths dont add up'\n",
    "\n",
    "\n",
    "\n",
    "df_labeled_valid['is_valid']=1\n",
    "df_labeled_trn['is_valid']=0\n",
    "df_unlabeled['is_valid']=0\n",
    "\n",
    "#create a dataset made of df_labeled_trn and df_unlabeled\n",
    "#this simulates situation where we have a much larger set of labeled data\n",
    "df_large=pd.concat([df_labeled_trn,df_unlabeled])\n",
    "\n",
    "print('larger set size:',np.shape(df_large)[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         sentiment                                            content  \\\n",
      "877259           4  Talk about Different  I like this one  ? http:...   \n",
      "1434762          4  Got a job at KFC, taking in some new hobbies, ...   \n",
      "139399           0  @HeathCastor I dunno? I never been to a club. ...   \n",
      "53857            0                                 My phone is dying    \n",
      "976136           4                  is watching ellen degeneres show    \n",
      "\n",
      "         str_len  is_valid  \n",
      "877259        62         0  \n",
      "1434762       84         0  \n",
      "139399       116         0  \n",
      "53857         18         0  \n",
      "976136        33         0  \n",
      "         sentiment                                            content  \\\n",
      "299472           0  Ditching school. Im gonna sleep. =P LOL. Watch...   \n",
      "611166           0  @bear_smooter Even with a bottle of wine? That...   \n",
      "777080           0  Running the M/V Donald Creppel *sigh* for the ...   \n",
      "1414226          4  @myMisericordia oH I would love to see the dog...   \n",
      "1109583          4  What a perfect day... Sunshine, 23'C, 4-5 Bft ...   \n",
      "\n",
      "         str_len  is_valid  \n",
      "299472        91         0  \n",
      "611166        58         0  \n",
      "777080       112         0  \n",
      "1414226       63         0  \n",
      "1109583      137         0  \n",
      "size of each sample: 12645\n"
     ]
    }
   ],
   "source": [
    "#For tri-training, we need 3 bootstrapped samples from the labeled train set\n",
    "#That is, we need random sampling with replacement\n",
    "sample_n=np.shape(df_labeled_trn)[0]\n",
    "\n",
    "df_labeled_trn_s1=df_labeled_trn.sample(n=sample_n,replace=True)\n",
    "df_labeled_trn_s2=df_labeled_trn.sample(n=sample_n,replace=True)\n",
    "df_labeled_trn_s3=df_labeled_trn.sample(n=sample_n,replace=True)\n",
    "\n",
    "#add in the validation set\n",
    "df_labeled_trn_s1=pd.concat([df_labeled_trn_s1,df_labeled_valid])\n",
    "df_labeled_trn_s2=pd.concat([df_labeled_trn_s2,df_labeled_valid])\n",
    "df_labeled_trn_s3=pd.concat([df_labeled_trn_s3,df_labeled_valid])\n",
    "df_large=pd.concat([df_large,df_labeled_valid])\n",
    "\n",
    "print(df_labeled_trn_s1.head())\n",
    "print(df_labeled_trn_s2.head())\n",
    "\n",
    "print('size of each sample:',np.shape(df_labeled_trn_s1)[0])\n",
    "\n",
    "assert np.shape(df_labeled_trn_s1)[0]==np.shape(df_labeled_trn_s2)[0]\n",
    "assert np.shape(df_labeled_trn_s1)[0]==np.shape(df_labeled_trn_s3)[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save all of our datasets!\n",
    "\n",
    "df_trn.to_csv(DATAPATH/'df_full.csv', index=False)\n",
    "df_large.to_csv(DATAPATH/'df_large.csv', index=False)\n",
    "\n",
    "df_labeled.to_csv(DATAPATH/'df_labeled.csv', index=False)\n",
    "df_unlabeled.to_csv(DATAPATH/'df_unlabeled.csv', index=False)\n",
    "\n",
    "df_labeled_trn.to_csv(DATAPATH/'train_labeled.csv', index=False)\n",
    "df_labeled_valid.to_csv(DATAPATH/'valid_labeled.csv', index=False)\n",
    "df_labeled_test.to_csv(DATAPATH/'test_labeled.csv', index=False)\n",
    "\n",
    "df_labeled_trn_s1.to_csv(DATAPATH/'train_labeled_s1.csv', index=False)\n",
    "df_labeled_trn_s2.to_csv(DATAPATH/'train_labeled_s2.csv', index=False)\n",
    "df_labeled_trn_s3.to_csv(DATAPATH/'train_labeled_s3.csv', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and run language model on the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set momentum values\n",
    "moms = (0.8,0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#language model data object\n",
    "data_lm = TextLMDataBunch.from_csv(DATAPATH, 'df_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.500430</td>\n",
       "      <td>4.261662</td>\n",
       "      <td>0.252461</td>\n",
       "      <td>14:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.348021</td>\n",
       "      <td>4.134992</td>\n",
       "      <td>0.265569</td>\n",
       "      <td>14:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.225672</td>\n",
       "      <td>4.066481</td>\n",
       "      <td>0.272583</td>\n",
       "      <td>14:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.155884</td>\n",
       "      <td>4.047857</td>\n",
       "      <td>0.274637</td>\n",
       "      <td>14:16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#This is using fast.ai's pretrained language model:\n",
    "#Model pretrained on wikitext103 dataset: \n",
    "    #https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/\n",
    "    #~100M tokens, ~270K vocab\n",
    "#Model based on Merity/Socher's AWD-LSTM quasi-recurrent neural network (QRNN)\n",
    "    #https://arxiv.org/abs/1708.02182\n",
    "learn = language_model_learner(data_lm, AWD_LSTM)\n",
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(4, slice(1e-2), moms=moms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save weights from language model\n",
    "learn.save_encoder('enc_LM_sent140')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and run first round of classification models for tri-training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This involves building and training three separate models, each based on one of the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create datasets for classification models\n",
    "\n",
    "#First some preprocessing for main training set:\n",
    "df_test=pd.read_csv(DATAPATH/'valid_labeled.csv')\n",
    "df_test['is_valid']=1\n",
    "df_tmp = pd.read_csv(DATAPATH/'train_labeled.csv')\n",
    "df_tmp['is_valid']=0\n",
    "df_tmp=pd.concat([df_tmp,df_test])\n",
    "df_tmp.to_csv(DATAPATH/'df_tmp.csv', index=False)\n",
    "tmplist=TextList.from_csv(DATAPATH, 'df_tmp.csv', cols='content', vocab=data_lm.train_ds.vocab)\n",
    "data_clas_main = (tmplist\n",
    "                .split_from_df(col='is_valid')\n",
    "                .label_from_df(cols='sentiment')#.add_test(test_list)\n",
    "                .databunch(bs=40))\n",
    "\n",
    "\n",
    "\n",
    "#Now for the samples...\n",
    "text_train_list_s1=TextList.from_csv(DATAPATH, 'train_labeled_s1.csv', cols='content', vocab=data_lm.train_ds.vocab)\n",
    "text_train_list_s2=TextList.from_csv(DATAPATH, 'train_labeled_s2.csv', cols='content', vocab=data_lm.train_ds.vocab)\n",
    "text_train_list_s3=TextList.from_csv(DATAPATH, 'train_labeled_s3.csv', cols='content', vocab=data_lm.train_ds.vocab)\n",
    "text_test_list=TextList.from_csv(DATAPATH, 'test_labeled.csv', cols='content', vocab=data_lm.train_ds.vocab)\n",
    "\n",
    "text_large_list=TextList.from_csv(DATAPATH, 'df_large.csv', cols='content', vocab=data_lm.train_ds.vocab)\n",
    "\n",
    "#The datasets for the 3 samples\n",
    "data_clas_s1 = (text_train_list_s1\n",
    "                .split_from_df(col='is_valid')\n",
    "                .label_from_df(cols='sentiment')\n",
    "                .add_test(text_test_list)\n",
    "                .databunch(bs=40))\n",
    "\n",
    "data_clas_s2 = (text_train_list_s2\n",
    "                .split_from_df(col='is_valid')\n",
    "                .label_from_df(cols='sentiment')\n",
    "                .add_test(text_test_list)\n",
    "                .databunch(bs=40))\n",
    "\n",
    "data_clas_s3 = (text_train_list_s3\n",
    "                .split_from_df(col='is_valid')\n",
    "                .label_from_df(cols='sentiment')\n",
    "                .add_test(text_test_list)\n",
    "                .databunch(bs=40))\n",
    "\n",
    "#Data for the much larger dataset\n",
    "data_clas_large = (text_large_list\n",
    "                .split_from_df(col='is_valid')\n",
    "                .label_from_df(cols='sentiment')\n",
    "                .add_test(text_test_list)\n",
    "                .databunch(bs=40))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.557100</td>\n",
       "      <td>0.473547</td>\n",
       "      <td>0.778674</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.538399</td>\n",
       "      <td>0.469144</td>\n",
       "      <td>0.786290</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.528870</td>\n",
       "      <td>0.458265</td>\n",
       "      <td>0.793011</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.522673</td>\n",
       "      <td>0.450327</td>\n",
       "      <td>0.797043</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Classification model based on Howard/Ruder's ULMFiT approach:\n",
    "    #https://arxiv.org/abs/1801.06146\n",
    "#Start with pretrained LM and fine-tune on dataset as above\n",
    "#Keep the LM's \"encoder\" spine, but replace the head of the NN with a new network that predicts class\n",
    "#This approach utilizes transfer learning to enhance the classifier's performance\n",
    "#Also using one_cycle superconvergence: modify the learning rate and momentum value during training\n",
    "    #Ramp up then down the LR, while ramping down then up the momentum\n",
    "    #Significantly reduces training time and improves performance\n",
    "    #From Leslie Smith's paper: https://arxiv.org/pdf/1803.09820.pdf\n",
    "\n",
    "learn = text_classifier_learner(data_clas_s1, AWD_LSTM)\n",
    "learn.load_encoder('enc_LM_sent140')#load in the weights from the LM encoder\n",
    "learn.fit_one_cycle(4, moms=moms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.504981</td>\n",
       "      <td>0.447752</td>\n",
       "      <td>0.795251</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.502337</td>\n",
       "      <td>0.441159</td>\n",
       "      <td>0.802419</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.485947</td>\n",
       "      <td>0.434843</td>\n",
       "      <td>0.806452</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.464925</td>\n",
       "      <td>0.430869</td>\n",
       "      <td>0.800179</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.478139</td>\n",
       "      <td>0.427724</td>\n",
       "      <td>0.801971</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.441130</td>\n",
       "      <td>0.423802</td>\n",
       "      <td>0.807796</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.435809</td>\n",
       "      <td>0.424800</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.436540</td>\n",
       "      <td>0.423292</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Unfreeze all the layers and continue training:\n",
    "#Above, all the layers except for the last layer is frozen\n",
    "    #Idea is that \"encoder\" spine from the LM is fairly well-tuned, so only need to train the classifier head\n",
    "#Once head has been trained, now we can train all the layers of the network\n",
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(8, slice(1e-5,1e-3), moms=moms)\n",
    "learn.save('s1_12e_unfreeze_001')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.553158</td>\n",
       "      <td>0.479018</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.529465</td>\n",
       "      <td>0.456275</td>\n",
       "      <td>0.788979</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.511834</td>\n",
       "      <td>0.449919</td>\n",
       "      <td>0.791219</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.511102</td>\n",
       "      <td>0.446709</td>\n",
       "      <td>0.795699</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.512473</td>\n",
       "      <td>0.443832</td>\n",
       "      <td>0.798835</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.496623</td>\n",
       "      <td>0.444081</td>\n",
       "      <td>0.794803</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.498999</td>\n",
       "      <td>0.439865</td>\n",
       "      <td>0.799731</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.480570</td>\n",
       "      <td>0.436788</td>\n",
       "      <td>0.801075</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.471058</td>\n",
       "      <td>0.432162</td>\n",
       "      <td>0.799283</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.449307</td>\n",
       "      <td>0.433056</td>\n",
       "      <td>0.804659</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.441979</td>\n",
       "      <td>0.432702</td>\n",
       "      <td>0.798387</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.438534</td>\n",
       "      <td>0.431688</td>\n",
       "      <td>0.799283</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Do same for s2\n",
    "learn = text_classifier_learner(data_clas_s2, AWD_LSTM)\n",
    "learn.load_encoder('enc_LM_sent140')#load in the weights from the LM encoder\n",
    "learn.fit_one_cycle(4, moms=moms)\n",
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(8, slice(1e-5,1e-3), moms=moms)\n",
    "learn.save('s2_12e_unfreeze_001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.562157</td>\n",
       "      <td>0.463909</td>\n",
       "      <td>0.788082</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.537904</td>\n",
       "      <td>0.463287</td>\n",
       "      <td>0.791219</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.531749</td>\n",
       "      <td>0.453772</td>\n",
       "      <td>0.801075</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.515903</td>\n",
       "      <td>0.447557</td>\n",
       "      <td>0.801523</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.505817</td>\n",
       "      <td>0.446676</td>\n",
       "      <td>0.799283</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.511284</td>\n",
       "      <td>0.441550</td>\n",
       "      <td>0.801075</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.505528</td>\n",
       "      <td>0.438899</td>\n",
       "      <td>0.801523</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.484266</td>\n",
       "      <td>0.431523</td>\n",
       "      <td>0.801523</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.476524</td>\n",
       "      <td>0.427838</td>\n",
       "      <td>0.803315</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.461210</td>\n",
       "      <td>0.424891</td>\n",
       "      <td>0.811828</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.437228</td>\n",
       "      <td>0.423725</td>\n",
       "      <td>0.808692</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.437958</td>\n",
       "      <td>0.423919</td>\n",
       "      <td>0.814964</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Do same for s3\n",
    "learn = text_classifier_learner(data_clas_s3, AWD_LSTM)\n",
    "learn.load_encoder('enc_LM_sent140')#load in the weights from the LM encoder\n",
    "learn.fit_one_cycle(4, moms=moms)\n",
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(8, slice(1e-5,1e-3), moms=moms)\n",
    "learn.save('s3_12e_unfreeze_001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.513515</td>\n",
       "      <td>0.425657</td>\n",
       "      <td>0.809588</td>\n",
       "      <td>07:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.514032</td>\n",
       "      <td>0.416941</td>\n",
       "      <td>0.812276</td>\n",
       "      <td>07:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.496843</td>\n",
       "      <td>0.411278</td>\n",
       "      <td>0.814516</td>\n",
       "      <td>07:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.498127</td>\n",
       "      <td>0.408943</td>\n",
       "      <td>0.819892</td>\n",
       "      <td>07:23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.473285</td>\n",
       "      <td>0.385465</td>\n",
       "      <td>0.823477</td>\n",
       "      <td>19:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.442043</td>\n",
       "      <td>0.363186</td>\n",
       "      <td>0.838262</td>\n",
       "      <td>18:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.396280</td>\n",
       "      <td>0.347553</td>\n",
       "      <td>0.848118</td>\n",
       "      <td>18:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.401837</td>\n",
       "      <td>0.337543</td>\n",
       "      <td>0.859767</td>\n",
       "      <td>18:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.386306</td>\n",
       "      <td>0.332238</td>\n",
       "      <td>0.861559</td>\n",
       "      <td>19:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.388812</td>\n",
       "      <td>0.331113</td>\n",
       "      <td>0.861111</td>\n",
       "      <td>19:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.383931</td>\n",
       "      <td>0.326626</td>\n",
       "      <td>0.864695</td>\n",
       "      <td>19:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.398179</td>\n",
       "      <td>0.325612</td>\n",
       "      <td>0.866935</td>\n",
       "      <td>18:28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Do same for large... \n",
    "#This simulates the situation where we have a much larger fully labeled dataset to work with\n",
    "#Performance of this model is just being used as a benchmark...\n",
    "    #it should be an upper bound for how well we can do with a semi-supervised approach\n",
    "learn = text_classifier_learner(data_clas_large, AWD_LSTM)\n",
    "learn.load_encoder('enc_LM_sent140')#load in the weights from the LM encoder\n",
    "learn.fit_one_cycle(4, moms=moms)\n",
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(8, slice(1e-5,1e-3), moms=moms)\n",
    "learn.save('large_12e_unfreeze_001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.561704</td>\n",
       "      <td>0.481441</td>\n",
       "      <td>0.781362</td>\n",
       "      <td>00:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.549204</td>\n",
       "      <td>0.465433</td>\n",
       "      <td>0.793907</td>\n",
       "      <td>00:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.536986</td>\n",
       "      <td>0.450241</td>\n",
       "      <td>0.802419</td>\n",
       "      <td>00:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.525893</td>\n",
       "      <td>0.447468</td>\n",
       "      <td>0.800179</td>\n",
       "      <td>00:12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.526163</td>\n",
       "      <td>0.442150</td>\n",
       "      <td>0.805108</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.520787</td>\n",
       "      <td>0.434077</td>\n",
       "      <td>0.810932</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.512583</td>\n",
       "      <td>0.430157</td>\n",
       "      <td>0.809140</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.480988</td>\n",
       "      <td>0.423372</td>\n",
       "      <td>0.808692</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.489114</td>\n",
       "      <td>0.422890</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.468415</td>\n",
       "      <td>0.418971</td>\n",
       "      <td>0.811380</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.475307</td>\n",
       "      <td>0.417309</td>\n",
       "      <td>0.813172</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.480229</td>\n",
       "      <td>0.418968</td>\n",
       "      <td>0.811380</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Do same for main (unsampled) training set... \n",
    "learn = text_classifier_learner(data_clas_main, AWD_LSTM)\n",
    "learn.load_encoder('enc_LM_sent140')#load in the weights from the LM encoder\n",
    "learn.fit_one_cycle(4, moms=moms)\n",
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(8, slice(1e-5,1e-3), moms=moms)\n",
    "learn.save('main_12e_unfreeze_001')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get predictions on unlabeled data from all three samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 920,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct a data structure for making predictions on the unlabeled data\n",
    "df_test=pd.read_csv(DATAPATH/'df_unlabeled.csv')\n",
    "df_test['is_valid']=1\n",
    "\n",
    "df_tmp = pd.read_csv(DATAPATH/'train_labeled_s1.csv')\n",
    "df_tmp=df_tmp[df_tmp['is_valid']==0]\n",
    "df_tmp=pd.concat([df_tmp,df_test])\n",
    "\n",
    "df_tmp.to_csv(DATAPATH/'df_tmp.csv', index=False)\n",
    "tmplist=TextList.from_csv(DATAPATH, 'df_tmp.csv', cols='content', vocab=data_lm.train_ds.vocab)\n",
    "data_tmp = (tmplist\n",
    "                .split_from_df(col='is_valid')\n",
    "                .label_from_df(cols='sentiment')\n",
    "                .databunch(bs=40))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 921,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make predictions on the unlabeled data, for all three samples\n",
    "learn = text_classifier_learner(data_tmp, AWD_LSTM)\n",
    "learn.load('s1_12e_unfreeze_001')\n",
    "predictions_s1=learn.get_preds(DatasetType.Valid,ordered=True)\n",
    "learn.load('s2_12e_unfreeze_001')\n",
    "predictions_s2=learn.get_preds(DatasetType.Valid,ordered=True)\n",
    "learn.load('s3_12e_unfreeze_001')\n",
    "predictions_s3=learn.get_preds(DatasetType.Valid,ordered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 922,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert prediction probabilities into np arrays, and concatenate\n",
    "predictions_probs=np.stack([predictions_s1[0].cpu().numpy()[:,0],\n",
    "                            predictions_s2[0].cpu().numpy()[:,0],\n",
    "                            predictions_s3[0].cpu().numpy()[:,0]],1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 923,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Do various pre-processing useful for deciding which unlabeled examples\n",
    "    # go to which samples\n",
    "\n",
    "#predictions for each of 3 samples\n",
    "predictions_predtargets=(predictions_probs>.5).astype(int)\n",
    "#average prediction\n",
    "ave_pred=np.mean(predictions_predtargets,axis=1)\n",
    "#get average probability over 3 samples:\n",
    "ave_prob=np.mean(predictions_probs,axis=1)\n",
    "#get standard deviation over 3 samples:\n",
    "std_prob=np.std(predictions_probs,axis=1)\n",
    "#get biggest differences bw ave of 2 and the third\n",
    "dif_prob_12m3=np.mean(predictions_probs[:,[0,1]],axis=1)-predictions_probs[:,2]\n",
    "dif_prob_13m2=np.mean(predictions_probs[:,[0,2]],axis=1)-predictions_probs[:,1]\n",
    "dif_prob_23m1=np.mean(predictions_probs[:,[1,2]],axis=1)-predictions_probs[:,0]\n",
    "#get individual samples probs\n",
    "s1_prediction_probs=predictions_probs[:,0]\n",
    "s2_prediction_probs=predictions_probs[:,1]\n",
    "s3_prediction_probs=predictions_probs[:,2]\n",
    "#get averages between pairings of 2 samples\n",
    "ave_prob12=np.mean(predictions_probs[:,[0,1]],axis=1)\n",
    "ave_prob13=np.mean(predictions_probs[:,[0,2]],axis=1)\n",
    "ave_prob23=np.mean(predictions_probs[:,[1,2]],axis=1)\n",
    "#set target to 1 or 0 based on ave_pred:\n",
    "ave_prob_target=np.round(ave_pred).astype('int')\n",
    "#get the indices of the max and min values\n",
    "probs_max_ind=np.argmax(predictions_probs,axis=1)\n",
    "probs_min_ind=np.argmin(predictions_probs,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 924,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standard deviation between three models prediction labels:  0.044066004\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([1.00000e+00, 2.00000e+00, 6.00000e+00, 5.00000e+00, 1.50000e+01, 2.70000e+01, 4.40000e+01, 6.80000e+01,\n",
       "        1.37000e+02, 2.30000e+02, 3.62000e+02, 5.66000e+02, 9.83000e+02, 1.51200e+03, 2.42700e+03, 3.83300e+03,\n",
       "        5.95000e+03, 9.25600e+03, 1.38740e+04, 2.14060e+04, 3.23460e+04, 5.03180e+04, 8.75960e+04, 1.68758e+05,\n",
       "        8.81120e+04, 6.10790e+04, 4.59570e+04, 3.57090e+04, 2.72180e+04, 2.08260e+04, 1.52500e+04, 1.12110e+04,\n",
       "        7.87000e+03, 5.46100e+03, 3.83300e+03, 2.48500e+03, 1.57200e+03, 1.05500e+03, 6.52000e+02, 3.60000e+02,\n",
       "        2.19000e+02, 1.76000e+02, 7.20000e+01, 4.40000e+01, 1.90000e+01, 1.30000e+01, 1.20000e+01, 7.00000e+00,\n",
       "        3.00000e+00, 2.00000e+00]),\n",
       " array([-0.559595, -0.53567 , -0.511745, -0.48782 , ...,  0.564884,  0.58881 ,  0.612735,  0.63666 ], dtype=float32),\n",
       " <a list of 50 Patch objects>)"
      ]
     },
     "execution_count": 924,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# the difference in predicted label between\n",
    "    #a single sample and the average of the other two samples\n",
    "#The vast majority of generated predictions are very similar between the three models \n",
    "print('standard deviation between three models prediction labels: ',np.mean(np.std(predictions_probs,axis=1)))\n",
    "plt.hist(dif_prob_12m3,50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 925,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([18668., 19357., 15489., 13325., ..., 13230., 14515., 16023., 14599.]),\n",
       " array([1.470389e-04, 1.014541e-02, 2.014377e-02, 3.014214e-02, ..., 9.699886e-01, 9.799870e-01, 9.899853e-01,\n",
       "        9.999837e-01], dtype=float32),\n",
       " <a list of 100 Patch objects>)"
      ]
     },
     "execution_count": 925,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#The distribution of the generated probabilities\n",
    "#The distribution has peaks at 0 and 1, since it is trained to discriminate Neg from Pos\n",
    "plt.hist(ave_prob,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continue tri-training with the unlabeled dataset's predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 926,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(728939, 13)\n"
     ]
    }
   ],
   "source": [
    "get_top_n=5000# Get top 10K examples (5K of '0' and 5K of '1') of most certain pair-predicted labels \n",
    "df_test=pd.read_csv(DATAPATH/'df_unlabeled.csv')\n",
    "df_test['ave_prob12']=ave_prob12\n",
    "df_test['ave_prob13']=ave_prob13\n",
    "df_test['ave_prob23']=ave_prob23\n",
    "df_test['dif_prob_23m1']=dif_prob_23m1\n",
    "df_test['dif_prob_13m2']=dif_prob_13m2\n",
    "df_test['dif_prob_12m3']=dif_prob_12m3\n",
    "df_test['s1_prediction_probs']=s1_prediction_probs\n",
    "df_test['s2_prediction_probs']=s2_prediction_probs\n",
    "df_test['s3_prediction_probs']=s3_prediction_probs\n",
    "print(np.shape(df_test))\n",
    "\n",
    "df_tmp = pd.read_csv(DATAPATH/'train_labeled_s1.csv')\n",
    "df_tmp.to_csv(DATAPATH/'train_labeled_s1_tmp.csv', index=False)\n",
    "df_tmp = pd.read_csv(DATAPATH/'train_labeled_s2.csv')\n",
    "df_tmp.to_csv(DATAPATH/'train_labeled_s2_tmp.csv', index=False)\n",
    "df_tmp = pd.read_csv(DATAPATH/'train_labeled_s3.csv')\n",
    "df_tmp.to_csv(DATAPATH/'train_labeled_s3_tmp.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 927,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for s1\n",
    "ave_prob_pair=np.array(df_test['ave_prob23'])\n",
    "dif_prob_pair_m_s=np.array(df_test['dif_prob_23m1'])\n",
    "s_prediction_probs=np.array(df_test['s1_prediction_probs'])\n",
    "csv_name='train_labeled_s1_tmp.csv'\n",
    "df_tmp = pd.read_csv(DATAPATH/csv_name)\n",
    "#set targets to the mean of pair\n",
    "ave_prob_target_tmp=np.round(ave_prob_pair).astype(int)\n",
    "ave_prob_target_tmp[ave_prob_target_tmp==0]=4\n",
    "ave_prob_target_tmp[ave_prob_target_tmp==1]=0\n",
    "print(np.shape(ave_prob_target_tmp))\n",
    "#get thresholds\n",
    "ave_prob_pair_sorted=np.sort(ave_prob_pair)\n",
    "low_thresh=ave_prob_pair_sorted[get_top_n]\n",
    "high_thresh=ave_prob_pair_sorted[len(ave_prob_pair_sorted)-get_top_n]\n",
    "get_these=np.zeros(np.shape(ave_prob_pair)[0])\n",
    "get_these[ave_prob_pair<low_thresh]=1\n",
    "get_these[ave_prob_pair>high_thresh]=1\n",
    "get_these=get_these.astype(bool)\n",
    "print(np.shape(ave_prob_target_tmp[get_these]))\n",
    "#get selections\n",
    "ave_prob_pair_tmp=ave_prob_pair[get_these]\n",
    "s_prediction_probs_tmp=s_prediction_probs[get_these]\n",
    "dif_prob_pair_m_s_tmp=dif_prob_pair_m_s[get_these]\n",
    "ave_prob_target_tmp=ave_prob_target_tmp[get_these]\n",
    "print(np.shape(ave_prob_pair_tmp))\n",
    "#modify df_test_tmp\n",
    "df_test_tmp=df_test.iloc[get_these]\n",
    "print(np.shape(df_test_tmp))\n",
    "df_test=df_test[~get_these]\n",
    "print(np.shape(df_test))\n",
    "df_test_tmp['sentiment']=ave_prob_target_tmp\n",
    "#df_test_tmp['sentiment'].iloc[get_these_s1]=s_prediction_preds[get_these_s1]\n",
    "df_test_tmp['is_valid']=0\n",
    "#create expanded data for further training of sample's model\n",
    "df_tmp=pd.concat([df_tmp,df_test_tmp])\n",
    "df_tmp.to_csv(DATAPATH/csv_name, index=False)\n",
    "tmplist=TextList.from_csv(DATAPATH, csv_name, cols='content', vocab=data_lm.train_ds.vocab)\n",
    "data_tmp = (tmplist\n",
    "                .split_from_df(col='is_valid')\n",
    "                .label_from_df(cols='sentiment')\n",
    "                .databunch(bs=40))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 928,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22644, 13)\n",
      "(718940, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.272075</td>\n",
       "      <td>0.426291</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>00:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.246473</td>\n",
       "      <td>0.429492</td>\n",
       "      <td>0.802867</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.253791</td>\n",
       "      <td>0.427987</td>\n",
       "      <td>0.806900</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.242967</td>\n",
       "      <td>0.428436</td>\n",
       "      <td>0.810036</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Continue training model using the pseudo-labels\n",
    "print(np.shape(df_tmp))\n",
    "print(np.shape(df_test))\n",
    "learn = text_classifier_learner(data_tmp, AWD_LSTM)\n",
    "learn.load('s1_12e_unfreeze_001')#load in the weights from the LM encoder\n",
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(4, slice(1e-5,1e-3), moms=moms)\n",
    "learn.save('s1_16e_tritrained_001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 929,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for s2\n",
    "ave_prob_pair=np.array(df_test['ave_prob13'])\n",
    "dif_prob_pair_m_s=np.array(df_test['dif_prob_13m2'])\n",
    "s_prediction_probs=np.array(df_test['s2_prediction_probs'])\n",
    "csv_name='train_labeled_s2_tmp.csv'\n",
    "df_tmp = pd.read_csv(DATAPATH/csv_name)\n",
    "#set targets to the mean of pair\n",
    "ave_prob_target_tmp=np.round(ave_prob_pair).astype(int)\n",
    "ave_prob_target_tmp[ave_prob_target_tmp==0]=4\n",
    "ave_prob_target_tmp[ave_prob_target_tmp==1]=0\n",
    "print(np.shape(ave_prob_target_tmp))\n",
    "#get thresholds\n",
    "ave_prob_pair_sorted=np.sort(ave_prob_pair)\n",
    "low_thresh=ave_prob_pair_sorted[get_top_n]\n",
    "high_thresh=ave_prob_pair_sorted[len(ave_prob_pair_sorted)-get_top_n]\n",
    "get_these=np.zeros(np.shape(ave_prob_pair)[0])\n",
    "get_these[ave_prob_pair<low_thresh]=1\n",
    "get_these[ave_prob_pair>high_thresh]=1\n",
    "get_these=get_these.astype(bool)\n",
    "print(np.shape(ave_prob_target_tmp[get_these]))\n",
    "#get selections\n",
    "ave_prob_pair_tmp=ave_prob_pair[get_these]\n",
    "s_prediction_probs_tmp=s_prediction_probs[get_these]\n",
    "dif_prob_pair_m_s_tmp=dif_prob_pair_m_s[get_these]\n",
    "ave_prob_target_tmp=ave_prob_target_tmp[get_these]\n",
    "print(np.shape(ave_prob_pair_tmp))\n",
    "#modify df_test_tmp\n",
    "df_test_tmp=df_test.iloc[get_these]\n",
    "print(np.shape(df_test_tmp))\n",
    "df_test=df_test[~get_these]\n",
    "print(np.shape(df_test))\n",
    "df_test_tmp['sentiment']=ave_prob_target_tmp\n",
    "#df_test_tmp['sentiment'].iloc[get_these_s1]=s_prediction_preds[get_these_s1]\n",
    "df_test_tmp['is_valid']=0\n",
    "#create expanded data for further training of sample's model\n",
    "df_tmp=pd.concat([df_tmp,df_test_tmp])\n",
    "df_tmp.to_csv(DATAPATH/csv_name, index=False)\n",
    "tmplist=TextList.from_csv(DATAPATH, csv_name, cols='content', vocab=data_lm.train_ds.vocab)\n",
    "data_tmp = (tmplist\n",
    "                .split_from_df(col='is_valid')\n",
    "                .label_from_df(cols='sentiment')\n",
    "                .databunch(bs=40))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 930,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22644, 13)\n",
      "(708941, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.268938</td>\n",
       "      <td>0.440734</td>\n",
       "      <td>0.797043</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.256114</td>\n",
       "      <td>0.439992</td>\n",
       "      <td>0.798835</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.245596</td>\n",
       "      <td>0.438050</td>\n",
       "      <td>0.798387</td>\n",
       "      <td>00:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.251346</td>\n",
       "      <td>0.438122</td>\n",
       "      <td>0.801971</td>\n",
       "      <td>00:39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Continue training model using the pseudo-labels\n",
    "print(np.shape(df_tmp))\n",
    "print(np.shape(df_test))\n",
    "learn = text_classifier_learner(data_tmp, AWD_LSTM)\n",
    "learn.load('s2_12e_unfreeze_001')#load in the weights from the LM encoder\n",
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(4, slice(1e-5,1e-3), moms=moms)\n",
    "learn.save('s2_16e_tritrained_001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 931,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for s3\n",
    "ave_prob_pair=np.array(df_test['ave_prob12'])\n",
    "dif_prob_pair_m_s=np.array(df_test['dif_prob_12m3'])\n",
    "s_prediction_probs=np.array(df_test['s3_prediction_probs'])\n",
    "csv_name='train_labeled_s3_tmp.csv'\n",
    "df_tmp = pd.read_csv(DATAPATH/csv_name)\n",
    "#set targets to the mean of pair\n",
    "ave_prob_target_tmp=np.round(ave_prob_pair).astype(int)\n",
    "ave_prob_target_tmp[ave_prob_target_tmp==0]=4\n",
    "ave_prob_target_tmp[ave_prob_target_tmp==1]=0\n",
    "print(np.shape(ave_prob_target_tmp))\n",
    "#get thresholds\n",
    "ave_prob_pair_sorted=np.sort(ave_prob_pair)\n",
    "low_thresh=ave_prob_pair_sorted[get_top_n]\n",
    "high_thresh=ave_prob_pair_sorted[len(ave_prob_pair_sorted)-get_top_n]\n",
    "get_these=np.zeros(np.shape(ave_prob_pair)[0])\n",
    "get_these[ave_prob_pair<low_thresh]=1\n",
    "get_these[ave_prob_pair>high_thresh]=1\n",
    "get_these=get_these.astype(bool)\n",
    "print(np.shape(ave_prob_target_tmp[get_these]))\n",
    "#get selections\n",
    "ave_prob_pair_tmp=ave_prob_pair[get_these]\n",
    "s_prediction_probs_tmp=s_prediction_probs[get_these]\n",
    "dif_prob_pair_m_s_tmp=dif_prob_pair_m_s[get_these]\n",
    "ave_prob_target_tmp=ave_prob_target_tmp[get_these]\n",
    "print(np.shape(ave_prob_pair_tmp))\n",
    "#modify df_test_tmp\n",
    "df_test_tmp=df_test.iloc[get_these]\n",
    "print(np.shape(df_test_tmp))\n",
    "df_test=df_test[~get_these]\n",
    "print(np.shape(df_test))\n",
    "df_test_tmp['sentiment']=ave_prob_target_tmp\n",
    "#df_test_tmp['sentiment'].iloc[get_these_s1]=s_prediction_preds[get_these_s1]\n",
    "df_test_tmp['is_valid']=0\n",
    "#create expanded data for further training of sample's model\n",
    "df_tmp=pd.concat([df_tmp,df_test_tmp])\n",
    "df_tmp.to_csv(DATAPATH/csv_name, index=False)\n",
    "tmplist=TextList.from_csv(DATAPATH, csv_name, cols='content', vocab=data_lm.train_ds.vocab)\n",
    "data_tmp = (tmplist\n",
    "                .split_from_df(col='is_valid')\n",
    "                .label_from_df(cols='sentiment')\n",
    "                .databunch(bs=40))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 932,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22644, 13)\n",
      "(698942, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.265480</td>\n",
       "      <td>0.425043</td>\n",
       "      <td>0.813620</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.253222</td>\n",
       "      <td>0.415999</td>\n",
       "      <td>0.803763</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.245343</td>\n",
       "      <td>0.423436</td>\n",
       "      <td>0.814068</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.237727</td>\n",
       "      <td>0.423912</td>\n",
       "      <td>0.810484</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Continue training model using the pseudo-labels\n",
    "print(np.shape(df_tmp))\n",
    "print(np.shape(df_test))\n",
    "learn = text_classifier_learner(data_tmp, AWD_LSTM)\n",
    "learn.load('s3_12e_unfreeze_001')#load in the weights from the LM encoder\n",
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(4, slice(1e-5,1e-3), moms=moms)\n",
    "learn.save('s3_16e_tritrained_001')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round 2 of predictions and tri-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 933,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct a data structure for making predictions on the unlabeled data\n",
    "df_test['is_valid']=1\n",
    "\n",
    "df_tmp = pd.read_csv(DATAPATH/'train_labeled_s1.csv')\n",
    "df_tmp=df_tmp[df_tmp['is_valid']==0]\n",
    "df_tmp=pd.concat([df_tmp,df_test])\n",
    "\n",
    "df_tmp.to_csv(DATAPATH/'df_tmp.csv', index=False)\n",
    "tmplist=TextList.from_csv(DATAPATH, 'df_tmp.csv', cols='content', vocab=data_lm.train_ds.vocab)\n",
    "data_tmp = (tmplist\n",
    "                .split_from_df(col='is_valid')\n",
    "                .label_from_df(cols='sentiment')\n",
    "                .databunch(bs=40))\n",
    "\n",
    "#make predictions on the unlabeled data, for all three samples\n",
    "learn = text_classifier_learner(data_tmp, AWD_LSTM)\n",
    "learn.load('s1_16e_tritrained_001')\n",
    "predictions_s1_2=learn.get_preds(DatasetType.Valid,ordered=True)\n",
    "learn.load('s2_16e_tritrained_001')\n",
    "predictions_s2_2=learn.get_preds(DatasetType.Valid,ordered=True)\n",
    "learn.load('s3_16e_tritrained_001')\n",
    "predictions_s3_2=learn.get_preds(DatasetType.Valid,ordered=True)\n",
    "\n",
    "#convert prediction probabilities into np arrays, and concatenate\n",
    "predictions_probs=np.stack([predictions_s1_2[0].cpu().numpy()[:,0],\n",
    "                            predictions_s2_2[0].cpu().numpy()[:,0],\n",
    "                            predictions_s3_2[0].cpu().numpy()[:,0]],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 934,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(698942, 13)\n"
     ]
    }
   ],
   "source": [
    "#df_test=pd.read_csv(DATAPATH/'df_unlabeled.csv')\n",
    "\n",
    "#get averages between pairings of 2 samples\n",
    "ave_prob12=np.mean(predictions_probs[:,[0,1]],axis=1)\n",
    "ave_prob13=np.mean(predictions_probs[:,[0,2]],axis=1)\n",
    "ave_prob23=np.mean(predictions_probs[:,[1,2]],axis=1)\n",
    "\n",
    "df_test['ave_prob12']=ave_prob12\n",
    "df_test['ave_prob13']=ave_prob13\n",
    "df_test['ave_prob23']=ave_prob23\n",
    "print(np.shape(df_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 935,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for s1\n",
    "ave_prob_pair=np.array(df_test['ave_prob23'])\n",
    "csv_name='train_labeled_s1_tmp.csv'\n",
    "df_tmp = pd.read_csv(DATAPATH/csv_name)\n",
    "#set targets to the mean of pair\n",
    "ave_prob_target_tmp=np.round(ave_prob_pair).astype(int)\n",
    "ave_prob_target_tmp[ave_prob_target_tmp==0]=4\n",
    "ave_prob_target_tmp[ave_prob_target_tmp==1]=0\n",
    "print(np.shape(ave_prob_target_tmp))\n",
    "#get thresholds\n",
    "ave_prob_pair_sorted=np.sort(ave_prob_pair)\n",
    "low_thresh=ave_prob_pair_sorted[get_top_n]\n",
    "high_thresh=ave_prob_pair_sorted[len(ave_prob_pair_sorted)-get_top_n]\n",
    "get_these=np.zeros(np.shape(ave_prob_pair)[0])\n",
    "get_these[ave_prob_pair<low_thresh]=1\n",
    "get_these[ave_prob_pair>high_thresh]=1\n",
    "get_these=get_these.astype(bool)\n",
    "print(np.shape(ave_prob_target_tmp[get_these]))\n",
    "#get selections\n",
    "ave_prob_pair_tmp=ave_prob_pair[get_these]\n",
    "ave_prob_target_tmp=ave_prob_target_tmp[get_these]\n",
    "print(np.shape(ave_prob_pair_tmp))\n",
    "#modify df_test_tmp\n",
    "df_test_tmp=df_test.iloc[get_these]\n",
    "print(np.shape(df_test_tmp))\n",
    "df_test=df_test[~get_these]\n",
    "print(np.shape(df_test))\n",
    "df_test_tmp['sentiment']=ave_prob_target_tmp\n",
    "#df_test_tmp['sentiment'].iloc[get_these_s1]=s_prediction_preds[get_these_s1]\n",
    "df_test_tmp['is_valid']=0\n",
    "#create expanded data for further training of sample's model\n",
    "df_tmp=pd.concat([df_tmp,df_test_tmp])\n",
    "df_tmp.to_csv(DATAPATH/csv_name, index=False)\n",
    "tmplist=TextList.from_csv(DATAPATH, csv_name, cols='content', vocab=data_lm.train_ds.vocab)\n",
    "data_tmp = (tmplist\n",
    "                .split_from_df(col='is_valid')\n",
    "                .label_from_df(cols='sentiment')\n",
    "                .databunch(bs=40))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 936,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32643, 13)\n",
      "(688943, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.184301</td>\n",
       "      <td>0.429720</td>\n",
       "      <td>0.812276</td>\n",
       "      <td>00:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.179923</td>\n",
       "      <td>0.464210</td>\n",
       "      <td>0.808244</td>\n",
       "      <td>00:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.152048</td>\n",
       "      <td>0.440930</td>\n",
       "      <td>0.811828</td>\n",
       "      <td>00:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.165068</td>\n",
       "      <td>0.441226</td>\n",
       "      <td>0.809140</td>\n",
       "      <td>00:50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Continue training model using the pseudo-labels\n",
    "print(np.shape(df_tmp))\n",
    "print(np.shape(df_test))\n",
    "learn = text_classifier_learner(data_tmp, AWD_LSTM)\n",
    "learn.load('s1_16e_tritrained_001')#load in the weights from the LM encoder\n",
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(4, slice(1e-5,1e-3), moms=moms)\n",
    "learn.save('s1_20e_tritrained_001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 937,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for s2\n",
    "ave_prob_pair=np.array(df_test['ave_prob13'])\n",
    "csv_name='train_labeled_s2_tmp.csv'\n",
    "df_tmp = pd.read_csv(DATAPATH/csv_name)\n",
    "#set targets to the mean of pair\n",
    "ave_prob_target_tmp=np.round(ave_prob_pair).astype(int)\n",
    "ave_prob_target_tmp[ave_prob_target_tmp==0]=4\n",
    "ave_prob_target_tmp[ave_prob_target_tmp==1]=0\n",
    "print(np.shape(ave_prob_target_tmp))\n",
    "#get thresholds\n",
    "ave_prob_pair_sorted=np.sort(ave_prob_pair)\n",
    "low_thresh=ave_prob_pair_sorted[get_top_n]\n",
    "high_thresh=ave_prob_pair_sorted[len(ave_prob_pair_sorted)-get_top_n]\n",
    "get_these=np.zeros(np.shape(ave_prob_pair)[0])\n",
    "get_these[ave_prob_pair<low_thresh]=1\n",
    "get_these[ave_prob_pair>high_thresh]=1\n",
    "get_these=get_these.astype(bool)\n",
    "print(np.shape(ave_prob_target_tmp[get_these]))\n",
    "#get selections\n",
    "ave_prob_pair_tmp=ave_prob_pair[get_these]\n",
    "ave_prob_target_tmp=ave_prob_target_tmp[get_these]\n",
    "print(np.shape(ave_prob_pair_tmp))\n",
    "#modify df_test_tmp\n",
    "df_test_tmp=df_test.iloc[get_these]\n",
    "print(np.shape(df_test_tmp))\n",
    "df_test=df_test[~get_these]\n",
    "print(np.shape(df_test))\n",
    "df_test_tmp['sentiment']=ave_prob_target_tmp\n",
    "#df_test_tmp['sentiment'].iloc[get_these_s1]=s_prediction_preds[get_these_s1]\n",
    "df_test_tmp['is_valid']=0\n",
    "#create expanded data for further training of sample's model\n",
    "df_tmp=pd.concat([df_tmp,df_test_tmp])\n",
    "df_tmp.to_csv(DATAPATH/csv_name, index=False)\n",
    "tmplist=TextList.from_csv(DATAPATH, csv_name, cols='content', vocab=data_lm.train_ds.vocab)\n",
    "data_tmp = (tmplist\n",
    "                .split_from_df(col='is_valid')\n",
    "                .label_from_df(cols='sentiment')\n",
    "                .databunch(bs=40))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 938,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32641, 13)\n",
      "(678946, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.187832</td>\n",
       "      <td>0.448729</td>\n",
       "      <td>0.799283</td>\n",
       "      <td>00:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.191983</td>\n",
       "      <td>0.443321</td>\n",
       "      <td>0.801075</td>\n",
       "      <td>00:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.170951</td>\n",
       "      <td>0.448432</td>\n",
       "      <td>0.801971</td>\n",
       "      <td>00:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.153393</td>\n",
       "      <td>0.450314</td>\n",
       "      <td>0.803315</td>\n",
       "      <td>00:51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Continue training model using the pseudo-labels\n",
    "print(np.shape(df_tmp))\n",
    "print(np.shape(df_test))\n",
    "learn = text_classifier_learner(data_tmp, AWD_LSTM)\n",
    "learn.load('s2_16e_tritrained_001')#load in the weights from the LM encoder\n",
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(4, slice(1e-5,1e-3), moms=moms)\n",
    "learn.save('s2_20e_tritrained_001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 939,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for s3\n",
    "ave_prob_pair=np.array(df_test['ave_prob12'])\n",
    "csv_name='train_labeled_s3_tmp.csv'\n",
    "df_tmp = pd.read_csv(DATAPATH/csv_name)\n",
    "#set targets to the mean of pair\n",
    "ave_prob_target_tmp=np.round(ave_prob_pair).astype(int)\n",
    "ave_prob_target_tmp[ave_prob_target_tmp==0]=4\n",
    "ave_prob_target_tmp[ave_prob_target_tmp==1]=0\n",
    "print(np.shape(ave_prob_target_tmp))\n",
    "#get thresholds\n",
    "ave_prob_pair_sorted=np.sort(ave_prob_pair)\n",
    "low_thresh=ave_prob_pair_sorted[get_top_n]\n",
    "high_thresh=ave_prob_pair_sorted[len(ave_prob_pair_sorted)-get_top_n]\n",
    "get_these=np.zeros(np.shape(ave_prob_pair)[0])\n",
    "get_these[ave_prob_pair<low_thresh]=1\n",
    "get_these[ave_prob_pair>high_thresh]=1\n",
    "get_these=get_these.astype(bool)\n",
    "print(np.shape(ave_prob_target_tmp[get_these]))\n",
    "#get selections\n",
    "ave_prob_pair_tmp=ave_prob_pair[get_these]\n",
    "ave_prob_target_tmp=ave_prob_target_tmp[get_these]\n",
    "print(np.shape(ave_prob_pair_tmp))\n",
    "#modify df_test_tmp\n",
    "df_test_tmp=df_test.iloc[get_these]\n",
    "print(np.shape(df_test_tmp))\n",
    "df_test=df_test[~get_these]\n",
    "print(np.shape(df_test))\n",
    "df_test_tmp['sentiment']=ave_prob_target_tmp\n",
    "#df_test_tmp['sentiment'].iloc[get_these_s1]=s_prediction_preds[get_these_s1]\n",
    "df_test_tmp['is_valid']=0\n",
    "#create expanded data for further training of sample's model\n",
    "df_tmp=pd.concat([df_tmp,df_test_tmp])\n",
    "df_tmp.to_csv(DATAPATH/csv_name, index=False)\n",
    "tmplist=TextList.from_csv(DATAPATH, csv_name, cols='content', vocab=data_lm.train_ds.vocab)\n",
    "data_tmp = (tmplist\n",
    "                .split_from_df(col='is_valid')\n",
    "                .label_from_df(cols='sentiment')\n",
    "                .databunch(bs=40))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 940,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32643, 13)\n",
      "(668947, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.190354</td>\n",
       "      <td>0.432078</td>\n",
       "      <td>0.802867</td>\n",
       "      <td>00:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.179874</td>\n",
       "      <td>0.439112</td>\n",
       "      <td>0.810036</td>\n",
       "      <td>00:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.149430</td>\n",
       "      <td>0.434203</td>\n",
       "      <td>0.814964</td>\n",
       "      <td>00:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.172851</td>\n",
       "      <td>0.431397</td>\n",
       "      <td>0.814068</td>\n",
       "      <td>00:51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Continue training model using the pseudo-labels\n",
    "print(np.shape(df_tmp))\n",
    "print(np.shape(df_test))\n",
    "learn = text_classifier_learner(data_tmp, AWD_LSTM)\n",
    "learn.load('s3_16e_tritrained_001')#load in the weights from the LM encoder\n",
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(4, slice(1e-5,1e-3), moms=moms)\n",
    "learn.save('s3_20e_tritrained_001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get predictions on test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 941,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data structure needed for getting predictions on the test set\n",
    "\n",
    "df_test_tmp=pd.read_csv(DATAPATH/'test_labeled.csv')\n",
    "df_test_tmp['is_valid']=1\n",
    "\n",
    "df_tmp = pd.read_csv(DATAPATH/'train_labeled_s1.csv')\n",
    "df_tmp=df_tmp[df_tmp['is_valid']==0]\n",
    "df_tmp=pd.concat([df_tmp,df_test_tmp])\n",
    "df_tmp.to_csv(DATAPATH/'df_tmp.csv', index=False)\n",
    "tmplist=TextList.from_csv(DATAPATH, 'df_tmp.csv', cols='content', vocab=data_lm.train_ds.vocab)\n",
    "data_tmp = (tmplist\n",
    "                .split_from_df(col='is_valid')\n",
    "                .label_from_df(cols='sentiment')#.add_test(test_list)\n",
    "                .databunch(bs=40))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 942,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now get predictions on test set for:\n",
    "    #1) all three added_sample's\n",
    "    #2) main ds\n",
    "    #3) large ds\n",
    "    \n",
    "learn = text_classifier_learner(data_tmp, AWD_LSTM)\n",
    "\n",
    "#3 added_samples\n",
    "learn.load('s1_20e_tritrained_001')\n",
    "predictions_added_s1=learn.get_preds(DatasetType.Valid,ordered=True)\n",
    "learn.load('s2_20e_tritrained_001')\n",
    "predictions_added_s2=learn.get_preds(DatasetType.Valid,ordered=True)\n",
    "learn.load('s3_20e_tritrained_001')\n",
    "predictions_added_s3=learn.get_preds(DatasetType.Valid,ordered=True)\n",
    "\n",
    "#main dataset\n",
    "learn.load('main_12e_unfreeze_001')\n",
    "predictions_main=learn.get_preds(DatasetType.Valid,ordered=True)\n",
    "\n",
    "#large dataset\n",
    "learn.load('large_12e_unfreeze_001')\n",
    "predictions_large=learn.get_preds(DatasetType.Valid,ordered=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 943,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7974)\n"
     ]
    }
   ],
   "source": [
    "#Accuracy on test set for the main unsampled data\n",
    "#This is just the final answer we'd get without using any SSL\n",
    "print(accuracy(*predictions_main))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 944,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8377)\n"
     ]
    }
   ],
   "source": [
    "#Accuracy on test set for the much larger dataset\n",
    "#This is the accuracy if we somehow got labels for the much larger 'unlabeled' dataset\n",
    "#It should provide an upper bound for what we could expect for SSL\n",
    "print(accuracy(*predictions_large))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 945,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8001)\n",
      "tensor(0.7956)\n",
      "tensor(0.8050)\n"
     ]
    }
   ],
   "source": [
    "#Accuracy on test set for each individual sample\n",
    "#This closely matches what we get on the main unsampled dataset\n",
    "print(accuracy(*predictions_added_s1))\n",
    "print(accuracy(*predictions_added_s2))\n",
    "print(accuracy(*predictions_added_s3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 946,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy across the 3 samples:  0.8027790228597041\n"
     ]
    }
   ],
   "source": [
    "ave_predictions=(np.array(predictions_added_s1[0])+np.array(predictions_added_s2[0])+np.array(predictions_added_s3[0]))/3\n",
    "argmax_pred=np.argmax(ave_predictions,axis=1)\n",
    "the_targets=np.array(predictions_added_s1[1])\n",
    "the_matches=np.equal(argmax_pred,the_targets).astype('int')\n",
    "print('Final accuracy across the 3 samples: ',np.mean(the_matches))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
